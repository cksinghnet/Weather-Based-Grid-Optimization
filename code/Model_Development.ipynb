{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4stAYxWm8w-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2d8746-14be-4a0c-c52c-97361ebbfd5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###!@0 START INIT ENVIRONMENT\n",
        "!ls /content/drive/MyDrive/spark-3.5.2-bin-hadoop3.tgz\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf /content/drive/MyDrive/spark-3.5.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.2-bin-hadoop3\"\n",
        "###!@0 END INIT ENVIRONMENT"
      ],
      "metadata": {
        "id": "-Agf_0iH8zum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7976c64c-963a-4ecf-d8a3-05cac3b3bf78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/spark-3.5.2-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "!rm -rf /content/data/*.csv\n",
        "!ln -s /content/drive/MyDrive/DES_Project/DataSet/weather_data/*.csv /content/data/"
      ],
      "metadata": {
        "id": "rJI28leQ9PwO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Weather and consumption Data\n",
        "\n",
        "csv_path1 = '/content/drive/MyDrive/DES_Project/DataSet/weather_data.csv'\n",
        "csv_path2 = '/content/drive/MyDrive/DES_Project/DataSet/electricity_consumption_delhi.csv'"
      ],
      "metadata": {
        "id": "JpYLNkidfXci"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###!@1 START OF PYSPARK INIT\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import SparkSession\n",
        "input_type = 'sample'\n",
        "spark = SparkSession.builder\\\n",
        "         .master(\"local\")\\\n",
        "         .appName(\"Colab\")\\\n",
        "         .config('spark.ui.port', '4050')\\\n",
        "         .getOrCreate()\n",
        "# Spark is ready to go within Colab!\n",
        "###!@1 END OF PYSPARK INIT"
      ],
      "metadata": {
        "id": "RgVoPUE0_Wpm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data from csv files\n",
        "\n",
        "weather_data = spark.read.csv(csv_path1, header=True, inferSchema=True)\n",
        "electricity_data = spark.read.csv(csv_path2, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "AJpBJ49vw2wV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import required Libraries\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col, count, when, isnan\n",
        "from pyspark.sql.functions import to_date\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import avg, date_trunc\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.types import DateType\n",
        "from datetime import timedelta, datetime"
      ],
      "metadata": {
        "id": "BWuojU6D9bVw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows\n",
        "\n",
        "electricity_data.show(5)\n",
        "weather_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-GbRuA4hy-b",
        "outputId": "60fbd1e7-5b31-4142-cd92-7b557354e108"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------+----------------+\n",
            "| City|               Date|Consumption (MW)|\n",
            "+-----+-------------------+----------------+\n",
            "|Delhi|2000-01-01 00:00:00|            2508|\n",
            "|Delhi|2000-01-01 00:01:00|            2537|\n",
            "|Delhi|2000-01-01 00:02:00|            2548|\n",
            "|Delhi|2000-01-01 00:03:00|            2509|\n",
            "|Delhi|2000-01-01 00:04:00|            2443|\n",
            "+-----+-------------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+-------------------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "| City|               Date|Temperature (C)|Feels Like (C)|Humidity (%)|Pressure (hPa)|Weather Description|Wind Speed (m/s)|Cloudiness (%)|Rain (1h mm)|            Sunrise|             Sunset|\n",
            "+-----+-------------------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "|Delhi|2000-01-01 00:00:00|          16.47|         15.25|          33|           997|               cold|            2.58|            62|        2.53|2000-01-01 06:30:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01 00:01:00|           6.83|          8.02|          58|           997|               cold|            3.95|             3|        1.89|2000-01-01 06:30:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01 00:02:00|            4.4|          3.47|          64|          1025|               cold|            3.17|            77|        4.61|2000-01-01 06:29:00|2000-01-01 18:32:00|\n",
            "|Delhi|2000-01-01 00:03:00|           14.4|         13.04|          39|           971|              clear|            1.28|             0|        1.24|2000-01-01 06:30:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01 00:04:00|          15.71|         17.24|          78|          1094|               haze|            2.88|            35|        3.19|2000-01-01 06:31:00|2000-01-01 18:32:00|\n",
            "+-----+-------------------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print schema\n",
        "\n",
        "electricity_data.printSchema()\n",
        "weather_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGeaw2SRh1At",
        "outputId": "3bc76d2f-461a-4ed4-c535-61c0a09e8c72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Consumption (MW): integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Temperature (C): double (nullable = true)\n",
            " |-- Feels Like (C): double (nullable = true)\n",
            " |-- Humidity (%): integer (nullable = true)\n",
            " |-- Pressure (hPa): integer (nullable = true)\n",
            " |-- Weather Description: string (nullable = true)\n",
            " |-- Wind Speed (m/s): double (nullable = true)\n",
            " |-- Cloudiness (%): integer (nullable = true)\n",
            " |-- Rain (1h mm): double (nullable = true)\n",
            " |-- Sunrise: timestamp (nullable = true)\n",
            " |-- Sunset: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data statistics\n",
        "\n",
        "electricity_data.describe().show()\n",
        "weather_data.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2sWdxXph3LH",
        "outputId": "796648eb-eea8-4df9-8cd5-766621fe92d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+------------------+\n",
            "|summary|    City|  Consumption (MW)|\n",
            "+-------+--------+------------------+\n",
            "|  count|13099680|          13099680|\n",
            "|   mean|    NULL| 5488.466762623209|\n",
            "| stddev|    NULL|1797.6923665615125|\n",
            "|    min|   Delhi|              2400|\n",
            "|    max|   Delhi|              8600|\n",
            "+-------+--------+------------------+\n",
            "\n",
            "+-------+--------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
            "|summary|    City|   Temperature (C)|    Feels Like (C)|      Humidity (%)|   Pressure (hPa)|Weather Description|  Wind Speed (m/s)|    Cloudiness (%)|      Rain (1h mm)|\n",
            "+-------+--------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
            "|  count|13066560|          13066560|          13066560|          13066560|         13066560|           13066560|          13066560|          13066560|          13066560|\n",
            "|   mean|    NULL|  27.1572413963582|27.157047366713638| 50.00623255087797|999.9914872774472|               NULL|2.5498795191697354|40.009825309798444|2.5163957889450908|\n",
            "| stddev|    NULL|11.078899763151334|11.138843975156785|13.291701375152897|58.01048357360235|               NULL|1.4144287886859623| 32.86525388440219|1.5249311888659247|\n",
            "|    min|   Delhi|               2.0|               0.0|                20|              900|              clear|               0.1|                 0|               0.0|\n",
            "|    max|   Delhi|              45.0|              47.0|                80|             1100|              windy|               5.0|               100|               6.0|\n",
            "+-------+--------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "\n",
        "def missing_values(df):\n",
        "    total_rows = df.count()\n",
        "    return df.select([\n",
        "       (count(when(isnan(c) | col(c).isNull(), c)) / total_rows).alias(f\"{c}_missing\") if dict(df.dtypes)[c] in [\"double\", \"float\"]\n",
        "       else (count(when(col(c).isNull(), c)) / total_rows).alias(f\"{c}_missing\") for c in df.columns])\n",
        "\n",
        "missing_values(electricity_data).show()\n",
        "\n",
        "missing_values(weather_data).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM8BIN5ih5WJ",
        "outputId": "32c1c219-decf-4251-befa-e90902d8b6bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+------------------------+\n",
            "|City_missing|Date_missing|Consumption (MW)_missing|\n",
            "+------------+------------+------------------------+\n",
            "|         0.0|         0.0|                     0.0|\n",
            "+------------+------------+------------------------+\n",
            "\n",
            "+------------+------------+-----------------------+----------------------+--------------------+----------------------+---------------------------+------------------------+----------------------+--------------------+---------------+--------------+\n",
            "|City_missing|Date_missing|Temperature (C)_missing|Feels Like (C)_missing|Humidity (%)_missing|Pressure (hPa)_missing|Weather Description_missing|Wind Speed (m/s)_missing|Cloudiness (%)_missing|Rain (1h mm)_missing|Sunrise_missing|Sunset_missing|\n",
            "+------------+------------+-----------------------+----------------------+--------------------+----------------------+---------------------------+------------------------+----------------------+--------------------+---------------+--------------+\n",
            "|         0.0|         0.0|                    0.0|                   0.0|                 0.0|                   0.0|                        0.0|                     0.0|                   0.0|                 0.0|            0.0|           0.0|\n",
            "+------------+------------+-----------------------+----------------------+--------------------+----------------------+---------------------------+------------------------+----------------------+--------------------+---------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the DataFrame name is `weather_data` and the column is `Date`\n",
        "weather_data = weather_data.withColumn(\"Date\", F.to_date(F.col(\"Date\")))\n",
        "\n",
        "# Show the resulting DataFrame to verify\n",
        "weather_data.show()\n",
        "\n",
        "# Assuming the DataFrame name is `electricity_data` and the column is `Date`\n",
        "electricity_data = electricity_data.withColumn(\"Date\", F.to_date(F.col(\"Date\")))\n",
        "\n",
        "# Show the resulting DataFrame to verify\n",
        "electricity_data.show()"
      ],
      "metadata": {
        "id": "4AKOs9h-dXgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbd2508-e2cf-4e71-eab3-d2981f5eb7d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "| City|      Date|Temperature (C)|Feels Like (C)|Humidity (%)|Pressure (hPa)|Weather Description|Wind Speed (m/s)|Cloudiness (%)|Rain (1h mm)|            Sunrise|             Sunset|\n",
            "+-----+----------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "|Delhi|2000-01-01|          16.47|         15.25|          33|           997|               cold|            2.58|            62|        2.53|2000-01-01 06:30:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01|           6.83|          8.02|          58|           997|               cold|            3.95|             3|        1.89|2000-01-01 06:30:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01|            4.4|          3.47|          64|          1025|               cold|            3.17|            77|        4.61|2000-01-01 06:29:00|2000-01-01 18:32:00|\n",
            "|Delhi|2000-01-01|           14.4|         13.04|          39|           971|              clear|            1.28|             0|        1.24|2000-01-01 06:30:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01|          15.71|         17.24|          78|          1094|               haze|            2.88|            35|        3.19|2000-01-01 06:31:00|2000-01-01 18:32:00|\n",
            "|Delhi|2000-01-01|           3.44|          3.86|          62|           916|              clear|            4.45|             0|        2.09|2000-01-01 06:32:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01|           2.98|          2.52|          49|          1083|               cold|            1.16|            14|        1.67|2000-01-01 06:32:00|2000-01-01 18:32:00|\n",
            "|Delhi|2000-01-01|          12.83|         13.72|          29|           978|              sunny|            4.33|            12|        3.64|2000-01-01 06:29:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01|           6.15|          7.43|          65|          1068|               cold|             2.0|            62|        2.63|2000-01-01 06:30:00|2000-01-01 18:32:00|\n",
            "|Delhi|2000-01-01|           21.2|         23.11|          47|           984|               cold|            0.42|             8|         2.8|2000-01-01 06:28:00|2000-01-01 18:31:00|\n",
            "|Delhi|2000-01-01|          10.77|         10.03|          55|          1019|              windy|            2.14|            69|        1.66|2000-01-01 06:29:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01|          16.02|         17.75|          46|          1062|               haze|            1.65|            34|        4.35|2000-01-01 06:29:00|2000-01-01 18:32:00|\n",
            "|Delhi|2000-01-01|          17.09|         16.23|          37|          1098|               haze|            2.49|            59|        5.22|2000-01-01 06:32:00|2000-01-01 18:30:00|\n",
            "|Delhi|2000-01-01|          16.43|         17.46|          49|          1042|              windy|            4.54|            99|        3.94|2000-01-01 06:31:00|2000-01-01 18:28:00|\n",
            "|Delhi|2000-01-01|           3.85|          3.19|          38|          1032|              clear|            4.08|             0|        0.47|2000-01-01 06:29:00|2000-01-01 18:31:00|\n",
            "|Delhi|2000-01-01|          13.98|         15.82|          41|          1040|               haze|            4.07|            46|         0.0|2000-01-01 06:30:00|2000-01-01 18:29:00|\n",
            "|Delhi|2000-01-01|          10.48|         10.47|          35|           981|               haze|            1.01|            87|        1.26|2000-01-01 06:31:00|2000-01-01 18:28:00|\n",
            "|Delhi|2000-01-01|           7.01|          6.28|          53|           994|               haze|            4.26|            87|        3.03|2000-01-01 06:30:00|2000-01-01 18:29:00|\n",
            "|Delhi|2000-01-01|          17.63|          18.2|          55|          1018|               cold|            0.93|            76|         0.0|2000-01-01 06:32:00|2000-01-01 18:32:00|\n",
            "|Delhi|2000-01-01|          18.69|         17.16|          74|           969|               cold|            0.67|            17|         1.6|2000-01-01 06:32:00|2000-01-01 18:29:00|\n",
            "+-----+----------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+----------+----------------+\n",
            "| City|      Date|Consumption (MW)|\n",
            "+-----+----------+----------------+\n",
            "|Delhi|2000-01-01|            2508|\n",
            "|Delhi|2000-01-01|            2537|\n",
            "|Delhi|2000-01-01|            2548|\n",
            "|Delhi|2000-01-01|            2509|\n",
            "|Delhi|2000-01-01|            2443|\n",
            "|Delhi|2000-01-01|            2566|\n",
            "|Delhi|2000-01-01|            2524|\n",
            "|Delhi|2000-01-01|            2495|\n",
            "|Delhi|2000-01-01|            2517|\n",
            "|Delhi|2000-01-01|            2531|\n",
            "|Delhi|2000-01-01|            2405|\n",
            "|Delhi|2000-01-01|            2530|\n",
            "|Delhi|2000-01-01|            2530|\n",
            "|Delhi|2000-01-01|            2450|\n",
            "|Delhi|2000-01-01|            2465|\n",
            "|Delhi|2000-01-01|            2505|\n",
            "|Delhi|2000-01-01|            2580|\n",
            "|Delhi|2000-01-01|            2568|\n",
            "|Delhi|2000-01-01|            2488|\n",
            "|Delhi|2000-01-01|            2521|\n",
            "+-----+----------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing 'Sunrise' and 'Sunset' columns\n",
        "weather_data = weather_data.drop(\"Sunrise\", \"Sunset\")\n",
        "\n",
        "# Aggregating numerical columns\n",
        "numerical_agg = weather_data.groupBy(\"Date\", \"City\").agg(\n",
        "    *[F.avg(F.col(col)).alias(col) for col in weather_data.columns if col not in [\"Date\", \"Weather Description\", \"City\"]]\n",
        ")\n",
        "\n",
        "# Finding the most frequent category for Weather Description\n",
        "weather_description_count = weather_data.groupBy(\"Date\", \"Weather Description\").count()\n",
        "\n",
        "# Using a window function to rank weather descriptions by count for each date\n",
        "window_spec = Window.partitionBy(\"Date\").orderBy(F.desc(\"count\"))\n",
        "most_frequent_description = weather_description_count.withColumn(\n",
        "    \"rank\", F.row_number().over(window_spec)\n",
        ").filter(F.col(\"rank\") == 1).select(\"Date\", \"Weather Description\")\n",
        "\n",
        "# Joining numerical aggregates with the most frequent Weather Description\n",
        "final_weather_data = numerical_agg.join(most_frequent_description, on=\"Date\", how=\"inner\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "final_weather_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QoNztf7q1sD",
        "outputId": "763f3765-06fa-4964-d15d-50e6b004a950"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "|      Date| City|   Temperature (C)|    Feels Like (C)|      Humidity (%)|    Pressure (hPa)|  Wind Speed (m/s)|    Cloudiness (%)|      Rain (1h mm)|Weather Description|\n",
            "+----------+-----+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "|2000-01-01|Delhi|13.414423611111115|13.423902777777807| 50.34166666666667|          998.3875|2.5098333333333325|           39.2875|2.4560208333333313|               cold|\n",
            "|2000-01-02|Delhi|13.526083333333345|13.521673611111115| 50.58958333333333|1000.7111111111111| 2.576916666666667| 41.62222222222222| 2.536604166666667|              windy|\n",
            "|2000-01-03|Delhi|13.324187499999963|13.342222222222222| 50.09236111111111| 999.5020833333333|2.5229791666666666| 41.74791666666667|2.5269236111111106|              sunny|\n",
            "|2000-01-04|Delhi| 13.48008333333335|13.453527777777746|49.921527777777776|1001.2701388888889| 2.575236111111107| 40.69444444444444| 2.535937500000003|              windy|\n",
            "|2000-01-05|Delhi|13.701166666666657|13.693111111111115| 49.71736111111111|1000.8513888888889| 2.548833333333334| 39.55277777777778|2.4426527777777753|              clear|\n",
            "|2000-01-06|Delhi|13.773472222222212|13.806972222222246| 49.89930555555556|1000.3319444444444|2.5101597222222165| 38.55833333333333|2.5319791666666687|              clear|\n",
            "|2000-01-07|Delhi| 13.21830555555552|13.204513888888876| 49.93611111111111|1000.6055555555556|2.5988750000000005| 39.59513888888889| 2.507618055555555|               cold|\n",
            "|2000-01-08|Delhi|13.650444444444467|13.627104166666673| 49.28263888888889| 998.6597222222222| 2.584493055555558|40.208333333333336|2.4570416666666635|              windy|\n",
            "|2000-01-09|Delhi|13.365270833333332|13.334708333333346| 50.72708333333333|1002.0041666666667| 2.568326388888885| 40.74583333333333| 2.546583333333328|              sunny|\n",
            "|2000-01-10|Delhi|13.809798611111098|13.834555555555546| 49.97430555555555|        1001.80625| 2.536902777777775| 40.87916666666667| 2.567104166666669|              sunny|\n",
            "|2000-01-11|Delhi|13.647812500000018|13.679402777777785|           49.8625| 999.7708333333334| 2.536909722222225| 40.44722222222222| 2.471409722222221|              windy|\n",
            "|2000-01-12|Delhi|13.230777777777767|13.246270833333337| 49.97222222222222|1000.2472222222223| 2.555194444444446|40.479166666666664|2.4745624999999984|               cold|\n",
            "|2000-01-13|Delhi| 13.27849305555559|13.306423611111109| 50.23888888888889| 999.2909722222222|2.4220416666666695|39.583333333333336|2.5307013888888865|              windy|\n",
            "|2000-01-14|Delhi|13.003284722222201|12.955965277777787| 50.05277777777778| 999.6784722222222| 2.510048611111113| 40.21944444444444| 2.469166666666666|              sunny|\n",
            "|2000-01-15|Delhi|13.596145833333354|13.658243055555547| 50.84583333333333| 999.4888888888889| 2.556951388888888| 38.87708333333333| 2.564965277777775|              clear|\n",
            "|2000-01-16|Delhi|13.689944444444455| 13.67572222222222|49.797916666666666|1001.1097222222222|2.5432916666666676| 39.59027777777778|  2.46826388888889|               cold|\n",
            "|2000-01-17|Delhi|13.450319444444457|13.474006944444461|50.076388888888886| 999.4847222222222| 2.584381944444444| 39.37847222222222|2.5044444444444465|              windy|\n",
            "|2000-01-18|Delhi|13.570972222222233|13.549118055555548|50.115972222222226| 999.3861111111111|2.5265833333333285| 40.09583333333333| 2.491305555555554|               haze|\n",
            "|2000-01-19|Delhi|13.470944444444457|13.502041666666663| 50.49722222222222|1002.2972222222222|2.5544791666666677|38.576388888888886| 2.567090277777774|              clear|\n",
            "|2000-01-20|Delhi|13.364819444444457| 13.38091666666665| 50.00208333333333|1000.2861111111112| 2.638104166666662| 38.84097222222222| 2.495312500000001|              clear|\n",
            "+----------+-----+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating numerical columns\n",
        "final_electricity_data = electricity_data.groupBy(\"Date\", \"City\").agg(\n",
        "    *[\n",
        "        F.avg(F.col(col)).alias(col)\n",
        "        for col in electricity_data.columns\n",
        "        if col not in [\"Date\", \"City\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "final_electricity_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fWp0cLAttt-",
        "outputId": "08db9e30-f454-4901-cfd9-3b05ba77763d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------------------+\n",
            "|      Date| City|  Consumption (MW)|\n",
            "+----------+-----+------------------+\n",
            "|2000-01-19|Delhi|2498.4215277777776|\n",
            "|2000-04-24|Delhi| 2501.273611111111|\n",
            "|2000-07-23|Delhi|2502.6097222222224|\n",
            "|2001-12-20|Delhi|2751.0645833333333|\n",
            "|2002-01-11|Delhi|2996.2527777777777|\n",
            "|2004-03-20|Delhi|3500.3458333333333|\n",
            "|2004-04-21|Delhi|        3498.66875|\n",
            "|2005-07-21|Delhi|3751.0715277777776|\n",
            "|2005-07-25|Delhi|3748.7444444444445|\n",
            "|2006-05-16|Delhi| 3998.720138888889|\n",
            "|2006-12-28|Delhi|4001.2791666666667|\n",
            "|2007-08-15|Delhi| 4250.251388888889|\n",
            "|2007-08-29|Delhi| 4252.347916666667|\n",
            "|2000-06-29|Delhi|2500.7805555555556|\n",
            "|2001-08-11|Delhi|2749.6319444444443|\n",
            "|2001-11-20|Delhi| 2751.246527777778|\n",
            "|2002-05-20|Delhi|3000.2805555555556|\n",
            "|2002-05-31|Delhi|3000.9861111111113|\n",
            "|2002-10-26|Delhi|3001.0430555555554|\n",
            "|2002-10-30|Delhi|3001.8118055555556|\n",
            "+----------+-----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the two DataFrames on the 'Date' column\n",
        "merged_data = final_weather_data.join(final_electricity_data, on=\"Date\", how=\"inner\")\n",
        "\n",
        "# Dropping one of the City columns, keeping only one City column\n",
        "merged_data = merged_data.drop(final_electricity_data.City)\n",
        "\n",
        "# Show the resulting merged DataFrame\n",
        "merged_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHXulgMF7xCo",
        "outputId": "11b6f2d9-428d-4d3e-b276-6debd1248834"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+\n",
            "|      Date| City|   Temperature (C)|    Feels Like (C)|      Humidity (%)|    Pressure (hPa)|  Wind Speed (m/s)|    Cloudiness (%)|      Rain (1h mm)|Weather Description|  Consumption (MW)|\n",
            "+----------+-----+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+\n",
            "|2000-01-01|Delhi|13.414423611111115|13.423902777777807| 50.34166666666667|          998.3875|2.5098333333333325|           39.2875|2.4560208333333313|               cold|2501.7951388888887|\n",
            "|2000-01-02|Delhi|13.526083333333345|13.521673611111115| 50.58958333333333|1000.7111111111111| 2.576916666666667| 41.62222222222222| 2.536604166666667|              windy|2499.1965277777776|\n",
            "|2000-01-03|Delhi|13.324187499999963|13.342222222222222| 50.09236111111111| 999.5020833333333|2.5229791666666666| 41.74791666666667|2.5269236111111106|              sunny| 2499.995138888889|\n",
            "|2000-01-04|Delhi| 13.48008333333335|13.453527777777746|49.921527777777776|1001.2701388888889| 2.575236111111107| 40.69444444444444| 2.535937500000003|              windy| 2500.559027777778|\n",
            "|2000-01-05|Delhi|13.701166666666657|13.693111111111115| 49.71736111111111|1000.8513888888889| 2.548833333333334| 39.55277777777778|2.4426527777777753|              clear|2501.2361111111113|\n",
            "|2000-01-06|Delhi|13.773472222222212|13.806972222222246| 49.89930555555556|1000.3319444444444|2.5101597222222165| 38.55833333333333|2.5319791666666687|              clear|2499.3152777777777|\n",
            "|2000-01-07|Delhi| 13.21830555555552|13.204513888888876| 49.93611111111111|1000.6055555555556|2.5988750000000005| 39.59513888888889| 2.507618055555555|               cold| 2503.684027777778|\n",
            "|2000-01-08|Delhi|13.650444444444467|13.627104166666673| 49.28263888888889| 998.6597222222222| 2.584493055555558|40.208333333333336|2.4570416666666635|              windy| 2498.598611111111|\n",
            "|2000-01-09|Delhi|13.365270833333332|13.334708333333346| 50.72708333333333|1002.0041666666667| 2.568326388888885| 40.74583333333333| 2.546583333333328|              sunny|          2498.275|\n",
            "|2000-01-10|Delhi|13.809798611111098|13.834555555555546| 49.97430555555555|        1001.80625| 2.536902777777775| 40.87916666666667| 2.567104166666669|              sunny|2502.4166666666665|\n",
            "|2000-01-11|Delhi|13.647812500000018|13.679402777777785|           49.8625| 999.7708333333334| 2.536909722222225| 40.44722222222222| 2.471409722222221|              windy|2501.7194444444444|\n",
            "|2000-01-12|Delhi|13.230777777777767|13.246270833333337| 49.97222222222222|1000.2472222222223| 2.555194444444446|40.479166666666664|2.4745624999999984|               cold|2502.8944444444446|\n",
            "|2000-01-13|Delhi| 13.27849305555559|13.306423611111109| 50.23888888888889| 999.2909722222222|2.4220416666666695|39.583333333333336|2.5307013888888865|              windy| 2499.891666666667|\n",
            "|2000-01-14|Delhi|13.003284722222201|12.955965277777787| 50.05277777777778| 999.6784722222222| 2.510048611111113| 40.21944444444444| 2.469166666666666|              sunny| 2497.961111111111|\n",
            "|2000-01-15|Delhi|13.596145833333354|13.658243055555547| 50.84583333333333| 999.4888888888889| 2.556951388888888| 38.87708333333333| 2.564965277777775|              clear|2500.4958333333334|\n",
            "|2000-01-16|Delhi|13.689944444444455| 13.67572222222222|49.797916666666666|1001.1097222222222|2.5432916666666676| 39.59027777777778|  2.46826388888889|               cold|2499.1847222222223|\n",
            "|2000-01-17|Delhi|13.450319444444457|13.474006944444461|50.076388888888886| 999.4847222222222| 2.584381944444444| 39.37847222222222|2.5044444444444465|              windy| 2499.009722222222|\n",
            "|2000-01-18|Delhi|13.570972222222233|13.549118055555548|50.115972222222226| 999.3861111111111|2.5265833333333285| 40.09583333333333| 2.491305555555554|               haze|2500.0222222222224|\n",
            "|2000-01-19|Delhi|13.470944444444457|13.502041666666663| 50.49722222222222|1002.2972222222222|2.5544791666666677|38.576388888888886| 2.567090277777774|              clear|2498.4215277777776|\n",
            "|2000-01-20|Delhi|13.364819444444457| 13.38091666666665| 50.00208333333333|1000.2861111111112| 2.638104166666662| 38.84097222222222| 2.495312500000001|              clear|2500.5208333333335|\n",
            "+----------+-----+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting date-related features from the Date column\n",
        "merged_data = merged_data.withColumn(\"Year\", F.year(F.col(\"Date\")))\n",
        "merged_data = merged_data.withColumn(\"Month\", F.month(F.col(\"Date\")))\n",
        "merged_data = merged_data.withColumn(\"DayOfWeek\", F.dayofweek(F.col(\"Date\")))\n",
        "\n",
        "# Prepare Historical Data\n",
        "# Use only Temperature (C) as the predictor\n",
        "assembler = VectorAssembler(inputCols=[\"Temperature (C)\"], outputCol=\"features\")\n",
        "temp_data = assembler.transform(merged_data).select(\"features\", F.col(\"Consumption (MW)\").alias(\"label\"))\n",
        "\n",
        "# Split into train and test sets\n",
        "train_data, test_data = temp_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Display the training and testing data counts\n",
        "print(f\"Training Data Count: {train_data.count()}\")\n",
        "print(f\"Testing Data Count: {test_data.count()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFcZcxX60CZZ",
        "outputId": "5e222f47-879b-402b-a1d9-ded7891515e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Count: 7333\n",
            "Testing Data Count: 1741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train the model using GBTRegressor\n",
        "# gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\", maxIter=200, maxDepth=10)\n",
        "# model = gbt.fit(train_data)"
      ],
      "metadata": {
        "id": "WSYHt7vX2NF1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "9d762fcb-c35e-4ad5-c268-39561481f2ec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4d68af5cc29b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model using GBTRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGBTRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxDepth=10, seed=42)\n",
        "model = rf.fit(train_data)"
      ],
      "metadata": {
        "id": "eA78AAfNhy4-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate model performance\n",
        "predictions = model.transform(test_data)\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dext6qaP2V3o",
        "outputId": "dea56e23-b426-46a0-ea40-7701ce80c1ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+-----------------+\n",
            "|            features|             label|       prediction|\n",
            "+--------------------+------------------+-----------------+\n",
            "|[12.935465277777762]|          7997.875|5601.378920092127|\n",
            "|[12.977972222222222]| 7999.406944444445|5601.378920092127|\n",
            "|[13.003284722222201]| 2497.961111111111|5601.378920092127|\n",
            "| [13.03618055555555]| 5002.302777777778|5601.378920092127|\n",
            "|[13.068527777777803]| 2500.334027777778|5601.378920092127|\n",
            "|[13.078625000000013]| 3497.509722222222|5601.378920092127|\n",
            "|[13.097798611111127]| 4751.114583333333|5601.378920092127|\n",
            "|[13.114430555555593]|3250.6993055555554|5601.378920092127|\n",
            "|[13.138423611111104]| 7749.614583333333|5601.378920092127|\n",
            "|[13.139916666666672]| 7500.459722222222|5601.378920092127|\n",
            "|[13.140618055555533]|3248.7208333333333|5601.378920092127|\n",
            "|[13.141555555555533]| 4750.328472222222|5601.378920092127|\n",
            "|[13.143111111111107]| 4250.352083333333|5601.378920092127|\n",
            "| [13.14550000000001]| 5251.978472222222|5601.378920092127|\n",
            "| [13.16050000000001]|        3999.40625|5601.378920092127|\n",
            "|[13.166930555555556]| 5249.235416666666|5601.378920092127|\n",
            "| [13.17382638888889]| 3745.748611111111|5601.378920092127|\n",
            "|[13.192465277777778]|            6248.3|5601.378920092127|\n",
            "| [13.20034027777776]| 6501.594444444445|5601.378920092127|\n",
            "|[13.202993055555583]|3501.1541666666667|5601.378920092127|\n",
            "+--------------------+------------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model on Test Data\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hqf70T0giJV",
        "outputId": "754b1355-257a-48a4-e5d3-4d8ec9c02fdd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on test data: 1762.6810338249086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Future Data\n",
        "future_csv_path = \"/content/drive/MyDrive/DES_Project/DataSet/Predict_Date.csv\"\n",
        "future_data = spark.read.csv(future_csv_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "kX_xzwwGMNkc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the features for future data from csv\n",
        "assembler = VectorAssembler(inputCols=[\"Temp\"], outputCol=\"features\")  # Replace \"Temp\" with the exact column name\n",
        "future_assembled = assembler.transform(future_data).select(\"Date\", \"Temp\", \"features\")"
      ],
      "metadata": {
        "id": "1j5nWs5iMNWD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast Consumption\n",
        "predictions = model.transform(future_assembled)"
      ],
      "metadata": {
        "id": "I549_jCxM-Pd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the required columns\n",
        "predicted_data = predictions.select(\n",
        "    \"Date\", \"Temp\", F.col(\"prediction\").alias(\"Forecasted Avg Consumption (MW)\")\n",
        ")"
      ],
      "metadata": {
        "id": "9_Wccn2cNL0R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a preview of the saved data\n",
        "predicted_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTMfBeftNYO_",
        "outputId": "fe8825bc-19b3-4c65-da5b-0653833789fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-------------------------------+\n",
            "|      Date|      Temp|Forecasted Avg Consumption (MW)|\n",
            "+----------+----------+-------------------------------+\n",
            "| 11/4/2024| 14.545566|              5236.020391956263|\n",
            "| 11/5/2024| 15.142779|              5236.020391956263|\n",
            "| 11/6/2024| 15.459622|              5236.020391956263|\n",
            "| 11/7/2024| 15.641195|              5236.020391956263|\n",
            "| 11/8/2024|  15.75013|              5236.020391956263|\n",
            "| 11/9/2024| 15.817327|              5236.020391956263|\n",
            "|11/10/2024| 15.859499|              5236.020391956263|\n",
            "|11/11/2024| 15.886254|              5236.020391956263|\n",
            "|11/12/2024| 15.903345|              5236.020391956263|\n",
            "|11/13/2024| 15.914311|              5236.020391956263|\n",
            "|11/14/2024|  15.92137|              5236.020391956263|\n",
            "|11/15/2024|  15.92592|              5236.020391956263|\n",
            "|11/16/2024| 15.928855|              5236.020391956263|\n",
            "|11/17/2024| 15.930751|              5236.020391956263|\n",
            "|11/18/2024| 15.931973|              5236.020391956263|\n",
            "|11/19/2024| 15.932766|              5236.020391956263|\n",
            "|11/20/2024|  15.93328|              5236.020391956263|\n",
            "|11/21/2024| 15.933611|              5236.020391956263|\n",
            "|11/22/2024|15.9338255|              5236.020391956263|\n",
            "|11/23/2024| 15.933964|              5236.020391956263|\n",
            "+----------+----------+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Predictions to a New CSV\n",
        "output_path = \"/content/drive/MyDrive/DES_Project/DataSet/predicted_consumption.csv\"  # Replace with desired output path\n",
        "predicted_data.write.csv(output_path, header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "o5hocoTnqrbE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate the Next 30 Days\n",
        "# last_date = merged_data.agg(F.max(\"Date\").alias(\"last_date\")).collect()[0][\"last_date\"]\n",
        "# start_date = last_date\n",
        "# future_dates_list = [(start_date + timedelta(days=i)) for i in range(1, 31)]\n",
        "# future_dates_df = spark.createDataFrame(future_dates_list, DateType()).toDF(\"Date\")"
      ],
      "metadata": {
        "id": "Gok536MS214B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Add date-derived features\n",
        "# future_dates_df = future_dates_df.withColumn(\"Year\", F.year(F.col(\"Date\")))\n",
        "# future_dates_df = future_dates_df.withColumn(\"Month\", F.month(F.col(\"Date\")))\n",
        "# future_dates_df = future_dates_df.withColumn(\"DayOfWeek\", F.dayofweek(F.col(\"Date\")))"
      ],
      "metadata": {
        "id": "iSCDupvH3Sua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Add placeholder values for weather features (adjust values as needed)\n",
        "# future_dates_df = future_dates_df.withColumn(\"Temperature (C)\", F.lit(25.0))  # Example value\n",
        "# future_dates_df = future_dates_df.withColumn(\"Humidity (%)\", F.lit(60.0))\n",
        "# future_dates_df = future_dates_df.withColumn(\"Wind Speed (m/s)\", F.lit(3.0))\n",
        "# future_dates_df = future_dates_df.withColumn(\"Cloudiness (%)\", F.lit(50.0))\n",
        "# future_dates_df = future_dates_df.withColumn(\"Rain (1h mm)\", F.lit(0.0))"
      ],
      "metadata": {
        "id": "7X2UT9F43V-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Prepare Future Data for Prediction\n",
        "# future_features = vector_assembler.transform(future_dates_df).select(\"Date\", \"features\")"
      ],
      "metadata": {
        "id": "QOchMaH8sl7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Forecast on Future Dates\n",
        "# future_predictions = model.transform(future_features)\n",
        "# forecast = future_predictions.select(\"Date\", \"prediction\").withColumnRenamed(\"prediction\", \"Forecasted Avg Consumption (MW)\")"
      ],
      "metadata": {
        "id": "zLYE7C1D3biA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Show the forecast for the next 30 days\n",
        "# forecast.show()"
      ],
      "metadata": {
        "id": "LsdzmBNx3deN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_T6jHhb3lvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}