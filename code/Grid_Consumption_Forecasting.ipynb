{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119b29d8",
   "metadata": {},
   "source": [
    "# Grid Consumption Forecasting with PySpark\n",
    "\n",
    "This notebook demonstrates how to forecast grid consumption using PySpark. It involves:\n",
    "- Loading and preprocessing weather and consumption data.\n",
    "- Feature engineering, including temporal and lagged features.\n",
    "- Training a Random Forest Regressor for prediction.\n",
    "- Evaluating the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209cf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, month, dayofweek, hour, unix_timestamp, from_unixtime\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f897602",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14326e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "         .master(\"local\")\\\n",
    "         .appName(\"Grid Consumption Forecasting\")\\\n",
    "         .config('spark.ui.port', '4050')\\\n",
    "         .config(\"spark.executor.memory\", \"4g\") \\\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbd9b0",
   "metadata": {},
   "source": [
    "## Load Weather and Consumption Data\n",
    "\n",
    "Load the weather and grid consumption data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470861a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code need to uncomment for local data read\n",
    "weather_file = \"../dataset/weather_data.csv\"\n",
    "consumption_file = \"../dataset/grid_consumption.csv\"\n",
    "\n",
    "weather_data = spark.read.csv(weather_file, header=True, inferSchema=True)\n",
    "consumption_data = spark.read.csv(consumption_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46287a",
   "metadata": {},
   "source": [
    "Load the weather and grid consumption data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# File paths to point to the mounted drive\n",
    "weather_file = \"/content/drive/MyDrive/Weather_Data/weather_data.csv\"\n",
    "consumption_file = \"/content/drive/MyDrive/Weather_Data/grid_consumption.csv\"\n",
    "\n",
    "weather_data = spark.read.csv(weather_file, header=True, inferSchema=True)\n",
    "consumption_data = spark.read.csv(consumption_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57034c",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Convert timestamps to Spark's timestamp format and round weather data to hourly timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'Date' column to timestamp and align\n",
    "weather_data = weather_data.withColumn(\"Date\", col(\"Date\").cast(\"timestamp\"))\n",
    "consumption_data = consumption_data.withColumn(\"Date\", col(\"Date\").cast(\"timestamp\"))\n",
    "weather_data = weather_data.withColumn(\"Date\", (unix_timestamp(\"Date\") / 3600).cast(\"int\") * 3600)\n",
    "\n",
    "# Convert weather_data.Date back to TIMESTAMP and join datasets\n",
    "weather_data = weather_data.withColumn(\"Date\", from_unixtime(col(\"Date\").cast(\"int\")))\n",
    "merged_data = consumption_data.join(weather_data, on=[\"City\", \"Date\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ac8e1",
   "metadata": {},
   "source": [
    "## Merge Datasets\n",
    "\n",
    "Join the weather and consumption datasets on `City` and `Date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data \\\n",
    "    .withColumn(\"Hour\", hour(col(\"Date\"))) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Date\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3393743",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Add temporal features and lagged consumption features for better modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"City\").orderBy(\"Date\")\n",
    "for lag_val in range(1, 25):\n",
    "    merged_data = merged_data.withColumn(f\"Lag_{lag_val}\", lag(\"Consumption (MW)\", lag_val).over(window_spec))\n",
    "\n",
    "merged_data = merged_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998abca0",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling\n",
    "\n",
    "Use a `VectorAssembler` to combine features into a single vector for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    \"Temperature (C)\", \"Feels Like (C)\", \"Humidity (%)\", \"Pressure (hPa)\",\n",
    "    \"Wind Speed (m/s)\", \"Cloudiness (%)\", \"Rain (1h mm)\", \"Hour\", \"DayOfWeek\", \"Month\"\n",
    "] + [f\"Lag_{lag_val}\" for lag_val in range(1, 25)]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(merged_data).select(\"features\", col(\"Consumption (MW)\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e956036",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2318bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f512e",
   "metadata": {},
   "source": [
    "## Train a Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomForest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Set up hyperparameter grid\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(rf.numTrees, [2, 2])\n",
    "              .addGrid(rf.maxDepth, [15, 20])\n",
    "              .build())\n",
    "\n",
    "# Cross-validator for model selection\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Train model using cross-validation\n",
    "cv_model = crossval.fit(train_data)\n",
    "model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47a6b8",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b155b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976e303",
   "metadata": {},
   "source": [
    "## Save the Model for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model using PySpark's persistence method\n",
    "model.write().overwrite().save(\"spark_rf_model\")\n",
    "\n",
    "print(f\"Model saved successfully with RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
